#!/usr/bin/env python

import pandas as pd
import numpy as np
import tensorflow as tf
import argparse
import pysam
import os, sys




parser = argparse.ArgumentParser(description = "Generating tensorflow data record files (DeepSea style)", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("--label", help = "Label file generated by 'generate_seq_labels.py'", required = True)
parser.add_argument("--bed", help = "Genome bed file - generated by 'generate_coordinate_onebed.py'", required = True)
parser.add_argument("--genome", help = "Genome fasta file - for extracting sequences", required = True)
parser.add_argument('--pad_scale', type=int, default = 5, help = 'scaling factor of window length: odd number input only. i.e. window 200, wanted seq length is 1000 => scale 5')
parser.add_argument("--out", default = "chr", help = "prefix tesorflow data record output file, each chromosome will output 2 files: chr_fw.tfr (forward strand) and chr_rc.tfr (reverse complement strand)")


args = parser.parse_args()

label = args.label
bed = args.bed
genome = args.genome
pad_scale = args.pad_scale
out = args.out

fw_out = out + '_fw.tfr'
rc_out = out + '_rc.tfr'

# test: ./generate_tfr.py --label label/1.txt.gz --bed genome_window.bed --genome /Users/datn/GENOMES/atlantic_salmon/Salmo_salar.Ssal_v3.1.dna.toplevel.fa --pad_scale 5

# ./generate_tfr.py --label label/25.txt.gz --bed genome_window.bed --genome /Users/datn/GENOMES/atlantic_salmon/Salmo_salar.Ssal_v3.1.dna.toplevel.fa --pad_scale 5 --out val_chr25.tfr

# label = 'label/1.txt.gz'
# bed = 'genome_window.bed'
# genome = '/Users/datn/GENOMES/atlantic_salmon/Salmo_salar.Ssal_v3.1.dna.toplevel.fa'
# pad_scale = 5
# out = 'out.tfr'

## check valid pad_scale
if pad_scale % 2 == 0:
    print(f'--pad_scale accept odd number only. Program exit!')
    exit()

pad = int((pad_scale-1)/2)

def read_bed_file(bed, chr):
    dic = {'c' : [], 's' : [], 'e' : [], 'id' : []} 
    f = open(bed, 'rt')
    for l in f:
        tem = l.split('\t')
        if tem[0] == chr:
            dic['c'].append(tem[0])
            dic['s'].append(int(tem[1]))
            dic['e'].append(int(tem[2]))
            dic['id'].append(tem[3].strip())
    f.close()
    return dic


## fast way to do one-hot encoding

embed = np.zeros([256, 4], np.float16)
embed[ord('A')] = np.array([1, 0, 0, 0])
embed[ord('C')] = np.array([0, 1, 0, 0])
embed[ord('G')] = np.array([0, 0, 1, 0])
embed[ord('T')] = np.array([0, 0, 0, 1])
embed[ord('a')] = np.array([1, 0, 0, 0])
embed[ord('c')] = np.array([0, 1, 0, 0])
embed[ord('g')] = np.array([0, 0, 1, 0])
embed[ord('t')] = np.array([0, 0, 0, 1])
embed[ord('.')] = np.array([.25, .25, .25, .25])
embed = tf.convert_to_tensor(embed)


def one_hot_encode_seq(dna_input, numpy = False,  name = "encode_seq"):
  with tf.name_scope(name):
    b = bytearray()
    b.extend(map(ord, str(dna_input)))
    t = tf.convert_to_tensor(b)
    t = tf.cast(t, tf.int32)
    encoded_dna = tf.nn.embedding_lookup(embed, t)
  if numpy == True: 
    return encoded_dna.numpy()
  else:
    return encoded_dna


## fast way to do reverse complement
tab = str.translate("ATGCatgc", "TACGtacg")
def reverse_complement_table(seq):
    return seq.translate(tab)[::-1]




def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """Returns a float_list from a float / double."""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature_np (values):
  """Convert numpy arrays to bytes features."""
  values = values.flatten().tobytes()
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))



# Create the features dictionary.
def seq_example(seq_1hot, label):
    feature = {
        'seq': _bytes_feature_np(seq_1hot),
        'label': _bytes_feature_np(label),
    }

    return tf.train.Example(features=tf.train.Features(feature=feature))


## process input data
print(f'Reading label file: {label}')
lab = pd.read_csv(label, sep = '\t', header = 0, index_col = 0)
# convert to dict for query with default
pos_lab = {idx : lab.loc[idx].to_numpy(dtype = np.int8) for idx in lab.index}
neg_lab = np.zeros(lab.shape[1], dtype = np.int8)

chr = lab.index[0].split('_')[0]

print(f'Reading bed file: {bed}')
mybed = read_bed_file(bed, chr)
N = len(mybed['id'])
begin = pad
end = N - pad
ref = pysam.FastaFile(genome)

## writer
tf_opts = tf.io.TFRecordOptions(compression_type='GZIP')


# forward
print(f'Write tensorflow data record file: {fw_out}')
count = 0
with tf.io.TFRecordWriter(fw_out, tf_opts) as writer:
  for i, id in enumerate(mybed['id']):
    if (i >= begin) and (i < end) :
      seq = ref.fetch(mybed['c'][i], mybed['s'][i-pad], mybed['e'][i+pad])
      seq_1hot = one_hot_encode_seq(seq, numpy = True)
      lab = pos_lab.get(id, neg_lab)
      tf_example = seq_example(seq_1hot, lab)
      writer.write(tf_example.SerializeToString())
      count += 1
      if count % 1000 == 0: print(f'done writing {count}/{end} records!') 



# reverse complement
print(f'Write tensorflow data record file: {rc_out}')
count = 0
with tf.io.TFRecordWriter(rc_out, tf_opts) as writer:
  for i, id in enumerate(mybed['id']):
    if (i >= begin) and (i < end) :
      seq = ref.fetch(mybed['c'][i], mybed['s'][i-pad], mybed['e'][i+pad])
      seq = reverse_complement_table(seq)
      seq_1hot = one_hot_encode_seq(seq, numpy = True)
      lab = pos_lab.get(id, neg_lab)
      tf_example = seq_example(seq_1hot, lab)
      writer.write(tf_example.SerializeToString())
      count += 1
      if count % 1000 == 0: print(f'done writing {count}/{end} records!') 












## not in use ==> for later decoding
# Decoding functions

record_file = rc_out

def parse_record(record):
  name_to_features = {
    'seq': tf.io.FixedLenFeature([], tf.string),
    'label': tf.io.FixedLenFeature([], tf.string),
  }
  return tf.io.parse_single_example(record, name_to_features)


def decode_record(record):
  seq = tf.io.decode_raw(
      record['seq'], out_type=tf.float16, little_endian=True, fixed_length=None, name=None
  )
  label = tf.io.decode_raw(
    record['label'], out_type=tf.int8, little_endian=True, fixed_length=None, name=None
  )
  seq = tf.reshape(seq, [-1,4])
  #label = tf.cast(label, tf.float16)
  return (seq, label)


def get_dataset(record_file, num_threads = 8, batch = 512):
    dataset = tf.data.TFRecordDataset(record_file, num_parallel_reads = num_threads, compression_type = 'GZIP')
    dataset = dataset.map(parse_record, num_parallel_calls = num_threads)
    dataset = dataset.map(decode_record, num_parallel_calls = num_threads)
    dataset = dataset.shuffle(buffer_size = batch*10).batch(batch)
    return dataset


# ## testing

# dataset = get_dataset(record_file,)

# it = iter(dataset)
# record = next(it)
# parsed_record = parse_record(record)
# x, y = decode_record(parsed_record)

# print(x.shape)
# print(y.shape)